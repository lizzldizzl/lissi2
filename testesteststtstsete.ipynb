{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Forensische Datenanalyse von Weblogs mit DuckLake (DuckDB)\n",
        "\n",
        "Dieses Notebook dokumentiert die forensische Untersuchung von Weblog-Daten zur Identifizierung und zum Beweis einer simulierten **Datenmanipulation**.\n",
        "\n",
        "**DuckLake** (eine Erweiterung für DuckDB) wird verwendet. DuckLake erstellt für jede Schreiboperation einen unveränderlichen **Snapshot** (Version). Hierfür wird das **Time Travel**-Feature von DuckLake genutzt, um unveränderliche Snapshots früherer Datenzustände zu rekonstruieren und so den Originalzustand mit dem manipulierten Zustand zu vergleichen (https://ducklake.select/docs/stable/duckdb/usage/time_travel.html)\n",
        "\n",
        "## Zweck und Ziel der Untersuchung\n",
        "\n",
        "Die Untersuchung verfolgt das primäre Ziel, die forensische Nachweisbarkeit von Datenmanipulation in Weblogs zu demonstrieren, indem das Time Travel-Feature von DuckLake genutzt wird, um manipulierte Datensätze anhand unveränderlicher Snapshots zu rekonstruieren und zu vergleichen.\n",
        "\n",
        "Ein weiteres Ziel ist der Performance-Vergleich von vier relevanten kryptografischen Verfahren (SHA-256, SHA3-256, AES-256, Blowfish), um ihre Eignung für die Datenintegrität (Hashing) und den vertraulichen Datenaustausch (Verschlüsselung) in realen Szenarien zu bewerten.\n",
        "\n",
        "Dabei wird die Integrität der Daten durch Hashing-Verfahren verifiziert, indem bereits kleinste Änderungen am Datensatz zu fundamental unterschiedlichen Hash-Werten führen, was als sofortiger Manipulationsnachweis dient.\n",
        "\n",
        "Parallel dazu wird die Vertraulichkeit der Daten durch die symmetrische Verschlüsselung gewährleistet und die Performance von modernen (AES) und traditionellen (Blowfish) Algorithmen anhand der gemessenen Ver- und Entschlüsselungszeiten analysiert.\n",
        "\n",
        "Zusammenfassend dient die Arbeit dazu, sowohl die technische Nachweisbarkeit von Cyber-Vorfällen als auch die fundierte Auswahl adäquater kryptografischer Schutzmaßnahmen in einer datenbankbasierten Umgebung zu evaluieren."
      ],
      "metadata": {
        "id": "TaZVVK_ibScm"
      },
      "id": "TaZVVK_ibScm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtZMQQyaJ6Dw"
      },
      "source": [
        "## 1. Vorbereitung und Daten-Setup"
      ],
      "id": "BtZMQQyaJ6Dw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importe einfügen"
      ],
      "metadata": {
        "id": "OR_S41ggfu6M"
      },
      "id": "OR_S41ggfu6M"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckdb\n",
        "!pip install pandas\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtl1dkjpgBn7",
        "outputId": "be511ccb-4efd-4b7f-c70e-24c734aa2f8a"
      },
      "id": "xtl1dkjpgBn7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "zLse3nkgfzKr"
      },
      "id": "zLse3nkgfzKr",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Code richtet eine persistente DuckDB-Datenbank ein und erstellt separate Verzeichnisse für Metadaten und Daten, um eine saubere Trennung zu gewährleisten. Anschließend wird die DuckLake-Erweiterung installiert, geladen und ein Katalog mit dem definierten Speicherort angebunden. Zum Schluss wird dieser Katalog als Standard gesetzt, sodass alle weiteren SQL-Operationen automatisch darin ausgeführt werden."
      ],
      "metadata": {
        "id": "df2K0132gJPf"
      },
      "id": "df2K0132gJPf"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Konfiguriere die Pfade\n",
        "# Verzeichnis für Metadaten und ein separates für die Daten (best practice)\n",
        "DUCKLAKE_METADATA_PATH = 'ducklake_metadata.ducklake'\n",
        "DUCKLAKE_DATA_PATH = 'ducklake_data_files'\n",
        "CATALOG_NAME = 'weblog_lake'\n",
        "\n",
        "# Verzeichnisse erstellen, falls sie noch nicht existieren\n",
        "os.makedirs(DUCKLAKE_DATA_PATH, exist_ok=True)\n",
        "\n",
        "# 2. Verbinde mit DuckDB\n",
        "# Die Verbindung zu einer permanenten DB ist für Langlebigkeit der Demo besser\n",
        "con = duckdb.connect(database='weblog_demo.duckdb')\n",
        "\n",
        "# DuckLake-Erweiterung\n",
        "con.sql(\"INSTALL ducklake;\")\n",
        "con.sql(\"LOAD ducklake;\")\n",
        "\n",
        "# 4. Attach DuckLake-Katalog\n",
        "attach_query = f\"\"\"\n",
        "    ATTACH 'ducklake:{DUCKLAKE_METADATA_PATH}' AS {CATALOG_NAME} (DATA_PATH '{DUCKLAKE_DATA_PATH}');\n",
        "\"\"\"\n",
        "con.sql(attach_query)\n",
        "\n",
        "# 5. DuckLake-Katalog als Standard setzen\n",
        "con.sql(f\"USE {CATALOG_NAME};\")\n",
        "\n",
        "print(f\"✅ DuckLake '{CATALOG_NAME}' erfolgreich erstellt und verbunden.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpxLvjO2KN3r",
        "outputId": "a7315f1c-acce-4ff1-a280-93302e28a336"
      },
      "id": "QpxLvjO2KN3r",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DuckLake 'weblog_lake' erfolgreich erstellt und verbunden.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code\n"
      ],
      "metadata": {
        "id": "hueOZcEOKQWs"
      },
      "id": "hueOZcEOKQWs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Link zum Datensatz\n",
        "https://www.kaggle.com/datasets/shawon10/web-log-dataset"
      ],
      "metadata": {
        "id": "3efCgDSYKYAY"
      },
      "id": "3efCgDSYKYAY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Datensatz enthält Server-Logdaten des RUET Online Judge (RUET OJ), einem Online-Bewertungssystem einer Universität. Er umfasst 16.008 Einträge mit vier Spalten: IP-Adresse, Zeitstempel, aufgerufene URL und HTTP-Statuscode. Ziel ist es, das Nutzerverhalten und die Serveraktivität zu analysieren, z. B. Login-Vorgänge oder Seitenaufrufe."
      ],
      "metadata": {
        "id": "IIBlWuVkKgq4"
      },
      "id": "IIBlWuVkKgq4"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle"
      ],
      "metadata": {
        "id": "BryWy7qgKirr"
      },
      "id": "BryWy7qgKirr",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ZHP2Rp0fKjst"
      },
      "id": "ZHP2Rp0fKjst",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Übergabe von Kaggle Benutzer Daten:\n",
        "{\"username\":\"....\",\"key\":\"....\"}"
      ],
      "metadata": {
        "id": "nSw0aobLKk6v"
      },
      "id": "nSw0aobLKk6v"
    },
    {
      "cell_type": "code",
      "source": [
        "# ... Hier Ihre JSON Cred als dictionary eingeben\n",
        "d_json_cred ={\"username\":\"lizzldizzl\",\"key\":\"7126d6d48a18986c8a8704fbb94e4a44\"}"
      ],
      "metadata": {
        "id": "OMV2j2CEKmUW"
      },
      "id": "OMV2j2CEKmUW",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle Zugangsdaten speichern\n",
        "\n"
      ],
      "metadata": {
        "id": "oBv7gpioKn96"
      },
      "id": "oBv7gpioKn96"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "kaggle_cred = pd.DataFrame(d_json_cred, index=[0]).to_json(\"~/.kaggle/kaggle.json\")"
      ],
      "metadata": {
        "id": "LzMIMM8BKqFb"
      },
      "id": "LzMIMM8BKqFb",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authorisierung geben dass Kaggle Daten heruntergeladen werden dürfen"
      ],
      "metadata": {
        "id": "04Eh0BGPKtLv"
      },
      "id": "04Eh0BGPKtLv"
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "O4W5oJl_KvAl"
      },
      "id": "O4W5oJl_KvAl",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d shawon10/web-log-dataset"
      ],
      "metadata": {
        "id": "PyCuBm0VKwPs"
      },
      "id": "PyCuBm0VKwPs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip der Daten"
      ],
      "metadata": {
        "id": "w6CnEfjhKyvQ"
      },
      "id": "w6CnEfjhKyvQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip web-log-dataset.zip -d ./data"
      ],
      "metadata": {
        "id": "6Y2ZCWwTK0wD"
      },
      "id": "6Y2ZCWwTK0wD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzN85hUPJ6Dx"
      },
      "source": [
        "## 2. DuckLake-Initialisierung und Snapshot 1 (Basis-Beweis)\n"
      ],
      "id": "LzN85hUPJ6Dx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Die Rohdaten werden in Pandas geladen, bereinigt und anschließend in den DuckLake-Katalog als erster unveränderlicher Zustand (Snapshot 1) eingefügt. Dies sichert den Original-Beweis."
      ],
      "metadata": {
        "id": "T0qNWNYtr-eK"
      },
      "id": "T0qNWNYtr-eK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CSCaPZ2J6Dy"
      },
      "outputs": [],
      "source": [
        "import duckdb, os\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime # Import datetime\n",
        "\n",
        "# --- Pfade & Namen ---\n",
        "LOG_PATH = './data/weblog.csv' # Pfad zur Logdatei\n",
        "DUCKLAKE_METADATA_PATH = 'forensic_evidence.ducklake'\n",
        "DUCKLAKE_DATA_PATH = 'forensic_data_files'\n",
        "CATALOG_NAME = 'forensic_log_archive'\n",
        "TABLE_NAME = 'access_logs'\n",
        "\n",
        "# Ordner erstellen\n",
        "os.makedirs(DUCKLAKE_DATA_PATH, exist_ok=True)\n",
        "\n",
        "# --- Datenbereinigung und Transformation ---\n",
        "try:\n",
        "    # 1. Daten einlesen\n",
        "    df = pd.read_csv(LOG_PATH)\n",
        "\n",
        "    # 2. Spaltennamen standardisieren\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # 3. Zeitstempel konvertieren (entfernt '[', nutzt Apache Log Format)\n",
        "    df['time'] = pd.to_datetime(\n",
        "        df['time'].astype(str).str.replace(\"[\", \"\", regex=False),\n",
        "        format=\"%d/%b/%Y:%H:%M:%S\",\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "\n",
        "    # 4. Spaltenumbenennung ('staus' -> 'status_code' und 'time' -> 'timestamp')\n",
        "    df.rename(columns={'time': 'timestamp', 'staus': 'status_code'}, inplace=True)\n",
        "\n",
        "    # 5. Statuscodes in robusten Integer-Typ konvertieren\n",
        "    df['status_code'] = pd.to_numeric(\n",
        "        df['status_code'],\n",
        "        errors='coerce'\n",
        "    ).astype(pd.Int64Dtype())\n",
        "\n",
        "    # 6. Finale Spaltenauswahl für die Archivierung\n",
        "    df = df[['timestamp', 'ip', 'url', 'status_code']]\n",
        "    print(f\"✅ Rohdaten erfolgreich bereinigt. {len(df)} Einträge bereit zur Archivierung.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"FEHLER: Datei nicht gefunden unter {LOG_PATH}. Bitte stellen Sie sicher, dass `weblog.csv` im Verzeichnis `./data/` liegt.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# --- DuckDB + DuckLake ---\n",
        "con = duckdb.connect('forensic_duckdb.db')\n",
        "con.sql(\"INSTALL ducklake;\")\n",
        "con.sql(\"LOAD ducklake;\")\n",
        "\n",
        "# Detach Katalog falls er bereits besteht\n",
        "try:\n",
        "    con.sql(f\"DETACH {CATALOG_NAME};\")\n",
        "except:\n",
        "    pass # Ignore Fehler falls Katalog ist nicht attached\n",
        "\n",
        "\n",
        "# Katalog anhängen und verwenden (definiert den Speicherort der Snapshots)\n",
        "attach_query = f\"\"\"\n",
        "    ATTACH 'ducklake:{DUCKLAKE_METADATA_PATH}' AS {CATALOG_NAME}\n",
        "    (DATA_PATH '{DUCKLAKE_DATA_PATH}');\n",
        "\"\"\"\n",
        "con.sql(attach_query)\n",
        "con.sql(f\"USE {CATALOG_NAME};\")\n",
        "\n",
        "print(f\"✅ Forensisches Log-Archiv '{CATALOG_NAME}' initialisiert.\")\n",
        "\n",
        "\n",
        "# --- Snapshot 1 erstellen ---\n",
        "con.sql(f\"CREATE OR REPLACE TABLE {TABLE_NAME} AS SELECT * FROM df LIMIT 0;\")\n",
        "con.sql(f\"INSERT INTO {TABLE_NAME} SELECT * FROM df;\")\n",
        "\n",
        "snapshot_info = con.sql(\n",
        "    f\"SELECT snapshot_id, snapshot_time \"\n",
        "    f\"FROM ducklake_snapshots('{CATALOG_NAME}') \"\n",
        "    f\"ORDER BY snapshot_id DESC LIMIT 1\"\n",
        ").fetchone()\n",
        "\n",
        "# Store snapshot_1_id for later use\n",
        "snapshot_1_id = snapshot_info[0]\n",
        "\n",
        "print(f\"\\n✅ Snapshot 1 (Original-Log) erfasst. ID: {snapshot_1_id}, Time: {snapshot_info[1]}\")"
      ],
      "id": "1CSCaPZ2J6Dy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OeLpvfeJ6Dy"
      },
      "source": [
        "## 3. Simulierte Log-Manipulation und Snapshot 2 (Manipuliertes Log)\n"
      ],
      "id": "5OeLpvfeJ6Dy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es wird simuliert, dass ein Angreifer kritische Log-Einträge entfernt und einen Ablenkungseintrag hinzufügt. Diese Aktionen führen zum **Snapshot 2**, dem manipulierten Zustand. Kritische Log- Einträge sind hierbei alle 400er Fehlercodes, die auf eine ungültige Anfrage hinweisen. So könnte der Angreifer versuchen, seine Zugriffe zu vertuschen. Der Fehler **404 (Not Found)** könnte z.B. auch auf **Directory Scans** (Angriffe zur Pfaderkennung) hinweisen, was der Angreifer vermeiden möchte (https://infosecwriteups.com/investigate-web-attacks-challenge-lets-defend-24ea96524290). Im besten Falle für ihn bleibt sein Angriff/ Datenabgriff erst einmal unbekannt."
      ],
      "metadata": {
        "id": "29l0G_fAsC3d"
      },
      "id": "29l0G_fAsC3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYwIxdrHJ6Dy"
      },
      "outputs": [],
      "source": [
        "# Kurze Pause für eindeutigen Snapshot-Zeitstempel\n",
        "time.sleep(1)\n",
        "\n",
        "# --- Simulierte Log-Manipulation (Beweisvertuschung) ---\n",
        "\n",
        "# 1. Löschen verdächtiger Einträge (Alle Fehler-Einträge >= 400 werden gelöscht)\n",
        "# Dies ist eine direkte SQL-Aktion, die das Log manipuliert.\n",
        "deleted_count = con.execute(f\"DELETE FROM {TABLE_NAME} WHERE status_code >= 400;\").fetchall()[0][0]\n",
        "print(f\"   -> {deleted_count} Fehler-Einträge (>= 400) wurden gelöscht (Simulierte Manipulation).\")\n",
        "\n",
        "# 2. Hinzufügen eines unverdächtigen Eintrags im 'Rohdaten'-Format (Ablenkung)\n",
        "\n",
        "# Um die Illusion der Rohdaten-Manipulation zu erzeugen, wird ein\n",
        "# DataFrame erstellt, der die SPALTEN-NAMEN des Originals nutzt ('Time', 'Ip', 'Status', etc.),\n",
        "# anstatt der bereits bereinigten Namen ('timestamp', 'status_code').\n",
        "\n",
        "# Der vorherige Code hat die Spalten jedoch bereits in Kleinbuchstaben umbenannt.\n",
        "# Daher nutzen werden hier BEREINIGTEN SPALTEN-NAMEN verwendet, aber die Werte\n",
        "# werden so ausagefüllt, als ob sie aus dem Rohdaten-Kontext stammen (z.B. ein normaler 200er-Status).\n",
        "\n",
        "new_log_df = pd.DataFrame({\n",
        "    'timestamp': [pd.to_datetime(datetime.now())],\n",
        "    'ip': ['203.0.113.5'],\n",
        "    'url': ['/index.html'],\n",
        "    'status_code': [200] # Numerische Statuscodes werden direkt unterstützt\n",
        "})\n",
        "con.sql(f\"INSERT INTO {TABLE_NAME} SELECT * FROM new_log_df;\")\n",
        "print(f\"   -> Ein neuer 200-Eintrag wurde hinzugefügt (Ablenkung).\")\n",
        "\n",
        "# --- Snapshot 2 erfassen ---\n",
        "# Dieser Snapshot speichert den manipulierten Zustand in der History.\n",
        "snapshot_2_id = con.sql(f\"SELECT max(snapshot_id) FROM ducklake_snapshots('{CATALOG_NAME}')\").fetchone()[0]\n",
        "print(f\"\\n✅ Snapshot 2 (Manipuliertes Log) erfasst. ID: {snapshot_2_id}\")\n",
        "\n",
        "con.sql(f\"SELECT snapshot_id, strftime(snapshot_time, '%Y-%m-%d %H:%M:%S') AS time FROM ducklake_snapshots('{CATALOG_NAME}') ORDER BY snapshot_id;\").show()"
      ],
      "id": "nYwIxdrHJ6Dy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Forensische Analyse: Beweissicherung mit DuckLake-Funktionen\n"
      ],
      "metadata": {
        "id": "y9LSEwliV5yi"
      },
      "id": "y9LSEwliV5yi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In dem Code werden vier forensische Analyseschritte mit DuckLake durchgeführt:\n",
        "Zuerst zeigt ducklake_snapshots die komplette Versionshistorie der Tabelle und macht sichtbar, wann Änderungen passiert sind.\n",
        "Danach rekonstruiert ducklake_table_deletions, welche Einträge zwischen zwei Snapshots gelöscht wurden, um Manipulationen oder Vertuschungen nachzuweisen.\n",
        "Anschließend zeigt ducklake_table_insertions, welche neuen Einträge hinzugefügt wurden, die als Ablenkung oder Verschleierung der Änderungen dienen könnten.\n",
        "Zuletzt ruft ducklake_table_changes alle Änderungen zwischen zwei Snapshots ab und listet neben den Tabellenspalten auch Snapshot-ID, Row-ID sowie den Änderungstyp (insert oder delete) auf (https://duckdb.org/docs/stable/core_extensions/ducklake.html)"
      ],
      "metadata": {
        "id": "EcDp84aSsLdr"
      },
      "id": "EcDp84aSsLdr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Die vier Hauptfunktionen der forensischen Analyse:\n",
        "\n",
        "# 1. ducklake_snapshots: Zeigt die gesamte Versionshistorie\n",
        "print(\"\\n[Beweis 1: Manipulationshistorie]\\n\")\n",
        "con.sql(f\"\"\"\n",
        "    SELECT\n",
        "        snapshot_id,\n",
        "        strftime(snapshot_time, '%Y-%m-%d %H:%M:%S') AS time,\n",
        "        changes\n",
        "    FROM\n",
        "        ducklake_snapshots('{CATALOG_NAME}')\n",
        "    ORDER BY\n",
        "        snapshot_id;\n",
        "\"\"\").show()\n",
        "print(\"Die Snapshots listen die vollständige **Transaktionshistorie** auf und dokumentieren exakt den **Originalzustand (1)** und den **manipulierten Zustand (2)** der Daten.\")\n",
        "\n",
        "# 2. ducklake_table_deletions: Zeigt die Vertuschung\n",
        "print(\"\\n[Beweis 2: Gelöschte Einträge (Vertuschung) zwischen Snapshot 1 und 2]\\n\")\n",
        "con.sql(f\"\"\"\n",
        "    SELECT\n",
        "        ip,\n",
        "        url,\n",
        "        status_code\n",
        "    FROM\n",
        "        ducklake_table_deletions('{CATALOG_NAME}', 'main', '{TABLE_NAME}', {snapshot_1_id}, {snapshot_2_id});\n",
        "\"\"\").show()\n",
        "print(\"Die Deletions-Funktion rekonstruiert exakt die von der Manipulation entfernten Beweismittel (alle Statuscodes >= 400).\")\n",
        "\n",
        "# 3. ducklake_table_insertions: Zeigt die Ablenkung\n",
        "print(\"\\n[Beweis 3: Neu hinzugefügte Einträge (Ablenkung) zwischen Snapshot 1 und 2]\\n\")\n",
        "con.sql(f\"\"\"\n",
        "    SELECT\n",
        "        ip,\n",
        "        url,\n",
        "        status_code\n",
        "    FROM\n",
        "        ducklake_table_insertions('{CATALOG_NAME}', 'main', '{TABLE_NAME}', {snapshot_1_id}, {snapshot_2_id});\n",
        "\"\"\").show()\n",
        "print(\"Die Ausgabe der zeigt alle Zeilen, die von den gelöschten Beweisen nicht betroffen waren, aber neu in den Datenbestand eingefügt werden mussten, um den neuen, manipulierten Zustand der Tabelle widerzuspiegeln.\")\n",
        "\n",
        "# 4. ducklake_table_changes: Zeigt alle Änderungen (Inserts & Deletes)\n",
        "print(\"\\n[Beweis 4: Alle Änderungen zwischen Snapshot 1 und 2]\\n\")\n",
        "con.sql(f\"\"\"\n",
        "    SELECT\n",
        "        snapshot_id,\n",
        "        rowid,\n",
        "        change_type,\n",
        "        ip,\n",
        "        url,\n",
        "        status_code\n",
        "    FROM\n",
        "        ducklake_table_changes(\n",
        "            '{CATALOG_NAME}',\n",
        "            'main',\n",
        "            '{TABLE_NAME}',\n",
        "            {snapshot_1_id},\n",
        "            {snapshot_2_id}\n",
        "        );\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"Die Changes-Funktion listet alle eingefügten und gelöschten Zeilen auf und kennzeichnet sie mit 'insert' oder 'delete'.\")"
      ],
      "metadata": {
        "id": "J2JuO3PVV681"
      },
      "id": "J2JuO3PVV681",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Forensische Statuscode-Analyse: Beweis der Datenmanipulation\n"
      ],
      "metadata": {
        "id": "5Hs-n4IbWEDo"
      },
      "id": "5Hs-n4IbWEDo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dieser Code vergleicht die **HTTP-Statuscode-Verteilung** des **aktuellen (manipulierten) Zustands** (Snapshot 2) mit den **gelöschten Einträgen** (die den Beweis der Manipulation darstellen). Es wird die `ducklake_table_deletions`-Funktion genutzt, um die forensischen Beweise direkt abzurufen. Die zwei Balkendiagrammen visualisieren einmal die manipulierte aktuelle Verteilung und einmal die gelöschten Fehlercodes als Beweis. Am Ende gibt der Code die forensisch relevanten gelöschten Statuscodes auch tabellarisch im Terminal aus."
      ],
      "metadata": {
        "id": "et1WY51fsPJA"
      },
      "id": "et1WY51fsPJA"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Daten für den \"Original (Beweis)\" Zustand abrufen ---\n",
        "\n",
        "# Die gelöschten Zeilen (deletions) entsprechen den Fehler-Einträgen,\n",
        "# die in der Original-Logdatei enthalten waren.\n",
        "df_deletions = con.execute(f\"\"\"\n",
        "    SELECT\n",
        "        CAST(status_code AS VARCHAR) AS status_code\n",
        "    FROM\n",
        "        ducklake_table_deletions('{CATALOG_NAME}', 'main', '{TABLE_NAME}', {snapshot_1_id}, {snapshot_2_id});\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "# Analyse der gelöschten Einträge (die kritisch sind)\n",
        "original_errors = df_deletions.groupby('status_code').size().reset_index(name='cnt')\n",
        "original_errors['state'] = 'Original (Gelöschte Fehler)'\n",
        "original_errors['version_id'] = 1\n",
        "\n",
        "\n",
        "# --- 2. Daten für den \"Aktuellen (Manipulierten)\" Zustand abrufen ---\n",
        "df_manipulated = con.execute(f\"\"\"\n",
        "    SELECT\n",
        "        CAST(status_code AS VARCHAR) AS status_code,\n",
        "        COUNT(*) AS cnt,\n",
        "        2 AS version_id,\n",
        "        'Manipuliert (Aktuell)' AS state\n",
        "    FROM\n",
        "        {TABLE_NAME}\n",
        "    GROUP BY\n",
        "        status_code\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "\n",
        "# --- 3. Visualisierung der relevanten Daten (Manipulation vs. Gelöschte Fehler) ---\n",
        "\n",
        "# Kombinieren der Fehler aus dem Originalzustand und dem aktuellen Zustand\n",
        "combined_analysis = pd.concat([original_errors, df_manipulated], ignore_index=True)\n",
        "\n",
        "# Funktion zur Farbbestimmung\n",
        "def get_colors(df):\n",
        "    colors = []\n",
        "    for s in df['status_code']:\n",
        "        try:\n",
        "            if int(s) >= 400:\n",
        "                colors.append('darkred')\n",
        "            else:\n",
        "                colors.append('green')\n",
        "        except:\n",
        "            colors.append('gray')\n",
        "    return colors\n",
        "\n",
        "df_original_errors = combined_analysis[combined_analysis['version_id'] == 1].copy()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Manipulierter Zustand (Aktuell)\n",
        "df_manipulated.plot.bar(x=\"status_code\", y=\"cnt\", legend=False, ax=axes[0], color=get_colors(df_manipulated))\n",
        "axes[0].set_title(\"HTTP-Statuscode Verteilung (Aktuell - Manipuliert)\")\n",
        "axes[0].set_ylabel(\"Anzahl der Anfragen\")\n",
        "axes[0].set_xlabel(\"Statuscode\")\n",
        "axes[0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Plot 2: Historischer Beweis (Nur die gelöschten Fehlercodes)\n",
        "df_original_errors.plot.bar(x=\"status_code\", y=\"cnt\", legend=False, ax=axes[1], color='darkred')\n",
        "axes[1].set_title(\"HTTP-Statuscode Verteilung (Forensischer Beweis: Gelöschte Fehler)\")\n",
        "axes[1].set_ylabel(\"Anzahl der Anfragen\")\n",
        "axes[1].set_xlabel(\"Statuscode\")\n",
        "axes[1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nStatuscode-Analyse der gelöschten Einträge (Forensischer Beweis):\")\n",
        "print(df_original_errors)"
      ],
      "metadata": {
        "id": "mGWtXmtoWFse"
      },
      "id": "mGWtXmtoWFse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welche IP-Adressen haben 404-Einträgen im Originalzustand verursacht?"
      ],
      "metadata": {
        "id": "pb9aCrYw1yty"
      },
      "id": "pb9aCrYw1yty"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Analyse: Von welchen IP-Adressen kamen die 404-Fehler im Originalzustand? ---\n",
        "\n",
        "df_404_ips = con.execute(f\"\"\"\n",
        "    SELECT\n",
        "        ip,\n",
        "        COUNT(*) AS cnt\n",
        "    FROM\n",
        "        ducklake_table_deletions('{CATALOG_NAME}', 'main', '{TABLE_NAME}', {snapshot_1_id}, {snapshot_2_id})\n",
        "    WHERE\n",
        "        status_code = 404\n",
        "    GROUP BY\n",
        "        ip\n",
        "    ORDER BY\n",
        "        cnt DESC\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"Top IP-Adressen mit 404-Fehlern (Originalzustand, forensischer Beweis):\")\n",
        "print(df_404_ips)\n",
        "\n",
        "# Visualisierung als Balkendiagramm\n",
        "df_404_ips.plot.bar(x=\"ip\", y=\"cnt\", legend=False, figsize=(10, 5), title=\"IP-Adressen mit 404-Fehlern (Forensischer Beweis)\")\n",
        "plt.xlabel(\"IP-Adresse\")\n",
        "plt.ylabel(\"Anzahl der 404-Fehler\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oLngJrod0JpD"
      },
      "id": "oLngJrod0JpD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Weitere beispielhafte Analysen bei einer forensischen Analyse\n"
      ],
      "metadata": {
        "id": "hyC7PTEoWSPc"
      },
      "id": "hyC7PTEoWSPc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unabhängig der Snapshots könnten diese Analysen Aufschluss über den Angriff bieten."
      ],
      "metadata": {
        "id": "376m4i4wsTkh"
      },
      "id": "376m4i4wsTkh"
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Häufig aufgerufene URLs\n",
        " Der Code ermittelt die Top 10 am häufigsten aufgerufenen URLs und gibt sie tabellarisch aus. Anschließend erstellt er ein Balkendiagramm, das die Aufrufzahlen dieser URLs gegenüberstellt. Dadurch wird sichtbar, welche Seiten besonders oft aufgerufen wurden und wie stark sie sich in der Verteilung unterscheiden."
      ],
      "metadata": {
        "id": "PDzVTELbpgBg"
      },
      "id": "PDzVTELbpgBg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Abfrage der Top 10 URLs aus dem aktuellen Zustand (Snapshot 2 - Manipuliert)\n",
        "top_urls = con.execute(f\"\"\"\n",
        "    SELECT\n",
        "        url,\n",
        "        COUNT(*) AS cnt\n",
        "    FROM\n",
        "        {TABLE_NAME}\n",
        "    GROUP BY\n",
        "        url\n",
        "    ORDER BY\n",
        "        cnt DESC\n",
        "    LIMIT 10\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"Top 10 URLs (Aktueller Zustand - Manipuliert):\")\n",
        "print(top_urls)\n",
        "\n",
        "# Visualisierung\n",
        "top_urls.plot.bar(x=\"url\", y=\"cnt\", legend=False, title=\"Top 10 URLs (Aktueller Zustand - Manipuliert)\")\n",
        "plt.xlabel(\"URL-Pfad\")\n",
        "plt.ylabel(\"Anzahl der Anfragen\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bs4komIqWTyJ"
      },
      "id": "Bs4komIqWTyJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "Viele **Login- oder Admin-Seiten** können auf **Brute-Force- oder Enumeration-Angriffe** hinweisen (https://www.fortinet.com/uk/resources/cyberglossary/brute-force-attack) ; (https://www.techtarget.com/searchsecurity/tip/What-enumeration-attacks-are-and-how-to-prevent-them)\n"
      ],
      "metadata": {
        "id": "UjE0freFowf9"
      },
      "id": "UjE0freFowf9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zeitliche Analyse (Requests pro Tag)\n",
        "\n",
        "Der Code zählt die Anzahl der Requests pro Tag aus der Logtabelle, filtert dabei alle Einträge heraus, die nach 2019 (dort gibt es kaum Einträge mehr) liegen und gibt die Ergebnisse tabellarisch aus. Anschließend wird ein Liniendiagramm erstellt, das den zeitlichen Verlauf der Requests bis einschließlich 2019 darstellt."
      ],
      "metadata": {
        "id": "EN3hcOmoWYMN"
      },
      "id": "EN3hcOmoWYMN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Abfrage der Requests pro Tag, gefiltert auf das Jahr 2019 und älter (<= 2019)\n",
        "requests_per_day = con.execute(f\"\"\"\n",
        "    SELECT\n",
        "        CAST(timestamp AS DATE) AS day,\n",
        "        COUNT(*) AS cnt\n",
        "    FROM\n",
        "        {TABLE_NAME}\n",
        "    WHERE\n",
        "        -- Filtert alle Einträge aus, deren Jahr größer als 2019 ist\n",
        "        EXTRACT(YEAR FROM timestamp) <= 2019\n",
        "    GROUP BY\n",
        "        day\n",
        "    ORDER BY\n",
        "        day\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"Requests pro Tag (Aktueller Zustand - Gefiltert auf <= 2019):\")\n",
        "print(requests_per_day)\n",
        "\n",
        "# Visualisierung des Liniendiagramms\n",
        "requests_per_day.plot(\n",
        "    x=\"day\",\n",
        "    y=\"cnt\",\n",
        "    kind=\"line\",\n",
        "    legend=False,\n",
        "    title=\"Requests pro Tag (bis einschließlich 2019)\"\n",
        ")\n",
        "plt.xlabel(\"Datum\")\n",
        "plt.ylabel(\"Anzahl der Requests\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G9e5gPq3WbF-"
      },
      "id": "G9e5gPq3WbF-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbXDa0QiJ6Dz"
      },
      "source": [
        "### Top IP-Adressen\n",
        "\n",
        "Der Code ermittelt die Top 10 IP-Adressen mit den meisten Anfragen und gibt sie tabellarisch aus. Danach stellt er diese Ergebnisse in einem Balkendiagramm dar, um die Verteilung der Anfragen pro IP visuell sichtbar zu machen."
      ],
      "id": "mbXDa0QiJ6Dz"
    },
    {
      "cell_type": "code",
      "source": [
        "top_ips_manipulated = con.execute(f\"\"\"\n",
        "    SELECT\n",
        "        ip,\n",
        "        COUNT(*) AS cnt\n",
        "    FROM\n",
        "        {TABLE_NAME}\n",
        "    GROUP BY\n",
        "        ip\n",
        "    ORDER BY\n",
        "        cnt DESC\n",
        "    LIMIT 10\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(\"Top 10 IPs (Aktueller, manipulierter Zustand):\")\n",
        "print(top_ips_manipulated)\n",
        "\n",
        "# Visualisierung\n",
        "top_ips_manipulated.plot.bar(x=\"ip\", y=\"cnt\", legend=False, title=\"Top 10 IPs (Aktueller Zustand - Manipuliert)\")\n",
        "plt.xlabel(\"IP-Adresse\")\n",
        "plt.ylabel(\"Anzahl der Anfragen\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pZJr6yynQxu5"
      },
      "id": "pZJr6yynQxu5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 7. Dokumentation und Abschluss\n"
      ],
      "metadata": {
        "id": "YECg-1QidJ7I"
      },
      "id": "YECg-1QidJ7I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nach Abschluss der forensischen Analyse werden die temporären Datenbank- und Dateisystemobjekte entfernt, um eine saubere Arbeitsumgebung zu gewährleisten."
      ],
      "metadata": {
        "id": "M_QZdgswdNWw"
      },
      "id": "M_QZdgswdNWw"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil # Import the shutil module\n",
        "import os # Import the os module\n",
        "\n",
        "# Schließe die DuckDB-Verbindung\n",
        "con.close()\n",
        "\n",
        "# Entferne die temporär erstellten forensischen Artefakte und Daten (DuckDB- und DuckLake-Dateien)\n",
        "if os.path.exists('forensic_duckdb.db'):\n",
        "    os.remove('forensic_duckdb.db')\n",
        "\n",
        "if os.path.exists(DUCKLAKE_METADATA_PATH):\n",
        "    os.remove(DUCKLAKE_METADATA_PATH)\n",
        "\n",
        "if os.path.exists(DUCKLAKE_DATA_PATH):\n",
        "    shutil.rmtree(DUCKLAKE_DATA_PATH)\n",
        "\n",
        "print(\"✅ Forensische Umgebung (Datenbank-Dateien und DuckLake-Katalog) erfolgreich bereinigt.\")"
      ],
      "metadata": {
        "id": "oI4L8ae6dOud"
      },
      "id": "oI4L8ae6dOud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hinweis zur Verwendung von generativer KI\n"
      ],
      "metadata": {
        "id": "3G-0QykC-IQp"
      },
      "id": "3G-0QykC-IQp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dieser Code wurde unter Verwendung eines KI-Modells (Google Gemini 2.5 Flash) zur Unterstützung entwickelt. Das KI-Modell wurde verwendet, um die Markdowns zu formatieren, Erklärungen bereitzustellen und Vorschläge für die Implementierung zu machen. Die endgültige Validierung, Anpassung und Verantwortung für die Korrektheit des Codes und der Erklärungen liegt bei der Autorin.\n"
      ],
      "metadata": {
        "id": "JEkkmi-U-Jge"
      },
      "id": "JEkkmi-U-Jge"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}