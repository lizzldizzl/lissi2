{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🕵️ Forensische Analyse von Weblogs mit DuckLake und DuckDB",
    "",
    "## Einführung: DuckLake als Unveränderliches Archiv (Immutable Ledger)",
    "",
    "Dieses Notebook demonstriert die Kernfunktionen der **DuckLake-Erweiterung** für DuckDB, indem es eine forensische Untersuchung von Weblog-Daten durchführt. DuckLake speichert jeden Zustand als **versionierten Snapshot**, was die Rekonstruktion des Originalzustands (Time Travel) ermöglicht.",
    "",
    "Wir simulieren eine **Log-Manipulation** (Löschen von Fehlereinträgen) und nutzen DuckLake, um:",
    "1.  **Time Travel** zu nutzen, um den Originalzustand wiederherzustellen.",
    "2.  **Statistische Analysen** (Statuscodes, Top-IPs, URLs) auf dem Original-Log durchzuführen.",
    "3.  Die Manipulation durch die **Änderungsverfolgung** (`ducklake_table_deletions`) zu beweisen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vorbereitung und Daten-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Daten-Setup: Laden der Weblogs vom unzippten Pfad ---\n",
    "LOG_DIR = './data/web-log-dataset/'\n",
    "\n",
    "try:\n",
    "    # Suche nach der CSV-Datei im unzippten Verzeichnis\n",
    "    log_files = glob.glob(LOG_DIR + '*.csv')\n",
    "    if not log_files:\n",
    "        raise FileNotFoundError(f\"Keine CSV-Datei in '{LOG_DIR}' gefunden.\")\n",
    "    \n",
    "    # Lade die gefundene Datei\n",
    "    df = pd.read_csv(log_files[0])\n",
    "    print(f\"Original-Daten geladen von: {log_files[0]} ({len(df)} Zeilen).\")\n",
    "    \n",
    "    # Spaltenumbenennung gemäß der Struktur: Ip, Time, URL, Status\n",
    "    df.rename(columns={'Ip': 'ip', 'Time': 'timestamp', 'URL': 'url', 'Status': 'status_code'}, errors='raise', inplace=True)\n",
    "    \n",
    "    # Datenbereinigung und Typumwandlung\n",
    "    df['status_code'] = df['status_code'].astype(int)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    # Nur die relevanten Spalten behalten, um Fehlern vorzubeugen\n",
    "    df = df[['timestamp', 'ip', 'url', 'status_code']]\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden der Daten: {e}\")\n",
    "    print(\"Bitte stellen Sie sicher, dass `!unzip web-log-dataset.zip -d ./data` ausgeführt wurde und die Spalten Ip, Time, URL, Status in der CSV vorhanden sind.\")\n",
    "    raise\n",
    "    \n",
    "print(f\"Verwendete Spalten: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DuckLake-Initialisierung und Snapshot 1 (Basis-Beweis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Forensische Pfade definieren ---\n",
    "DUCKLAKE_METADATA_PATH = 'forensic_evidence.ducklake'\n",
    "DUCKLAKE_DATA_PATH = 'forensic_data_files'\n",
    "CATALOG_NAME = 'forensic_log_archive'\n",
    "table_name = 'access_logs'\n",
    "\n",
    "# Lokale Ordner erstellen \n",
    "os.makedirs(DUCKLAKE_DATA_PATH, exist_ok=True)\n",
    "\n",
    "# --- DuckDB initialisieren und DuckLake Extension laden ---\n",
    "con = duckdb.connect(database='forensic_duckdb.db')\n",
    "con.sql(\"INSTALL ducklake;\")\n",
    "con.sql(\"LOAD ducklake;\")\n",
    "\n",
    "# --- DuckLake Katalog anhängen und verwenden ---\n",
    "attach_query = f\"\"\"\n",
    "    ATTACH 'ducklake:{DUCKLAKE_METADATA_PATH}' AS {CATALOG_NAME} (DATA_PATH '{DUCKLAKE_DATA_PATH}');\n",
    "\"\"\"\n",
    "con.sql(attach_query)\n",
    "con.sql(f\"USE {CATALOG_NAME};\")\n",
    "\n",
    "print(f\"✅ Forensisches Log-Archiv '{CATALOG_NAME}' initialisiert.\")\n",
    "\n",
    "# --- Snapshot 1: Basis-Log (Original-Daten) ---\n",
    "# 1. Leere Tabelle mit Schema aus dem Pandas DF erstellen\n",
    "con.sql(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df LIMIT 0;\")\n",
    "\n",
    "# 2. Daten aus dem Pandas DataFrame einfügen (erzeugt Snapshot 1)\n",
    "con.sql(f\"INSERT INTO {table_name} SELECT * FROM df;\")\n",
    "\n",
    "# 3. Snapshot-ID und Zeitstempel erfassen (WICHTIG für Time Travel)\n",
    "snapshot_1_id = con.sql(f\"SELECT max(snapshot_id) FROM ducklake_snapshots('{CATALOG_NAME}')\").fetchone()[0]\n",
    "snapshot_1_time = con.sql(f\"SELECT snapshot_time FROM ducklake_snapshots('{CATALOG_NAME}') WHERE snapshot_id = {snapshot_1_id}\").fetchone()[0]\n",
    "\n",
    "print(f\"\\n✅ Snapshot 1 (Original-Log) erfasst. ID: {snapshot_1_id}, Time: {snapshot_1_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulierte Log-Manipulation und Snapshot 2 (Manipuliertes Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurze Pause für eindeutigen Snapshot-Zeitstempel\n",
    "time.sleep(1)\n",
    "\n",
    "# --- Simulierte Log-Manipulation (Beweisvertuschung) ---\n",
    "\n",
    "# Löschen verdächtiger Einträge (Alle Fehler-Einträge >= 400 werden gelöscht)\n",
    "deleted_count = con.execute(f\"DELETE FROM {table_name} WHERE status_code >= 400;\").fetchall()[0][0]\n",
    "print(f\"   -> {deleted_count} Fehler-Einträge (>= 400) wurden gelöscht (Simulierte Manipulation).\")\n",
    "\n",
    "# Hinzufügen eines unverdächtigen Eintrags (Ablenkung)\n",
    "new_log_df = pd.DataFrame({\n",
    "    'timestamp': [pd.to_datetime(datetime.now())], \n",
    "    'ip': ['203.0.113.5'], \n",
    "    'url': ['/normal/status'],\n",
    "    'status_code': [200]\n",
    "})\n",
    "con.sql(f\"INSERT INTO {table_name} SELECT * FROM new_log_df;\")\n",
    "print(f\"   -> Ein neuer 200-Eintrag wurde hinzugefügt (Ablenkung).\")\n",
    "\n",
    "# Snapshot 2 erfassen\n",
    "snapshot_2_id = con.sql(f\"SELECT max(snapshot_id) FROM ducklake_snapshots('{CATALOG_NAME}')\").fetchone()[0]\n",
    "print(f\"\\n✅ Snapshot 2 (Manipuliertes Log) erfasst. ID: {snapshot_2_id}\")\n",
    "\n",
    "con.sql(f\"SELECT snapshot_id, strftime(snapshot_time, '%Y-%m-%d %H:%M:%S') AS time FROM ducklake_snapshots('{CATALOG_NAME}') ORDER BY snapshot_id;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forensische Analyse: Time Travel & Änderungsverfolgung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Direkter Beweis: Gelöschte und hinzugefügte Einträge (`ducklake_table_deletions`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Forensischer Beweis: Gelöschte Einträge (Vertuschung) zwischen Snapshot 1 und 2]\\n\")\n",
    "con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        ip, \n",
    "        url,\n",
    "        status_code \n",
    "    FROM \n",
    "        ducklake_table_deletions('{CATALOG_NAME}', 'main', '{table_name}', {snapshot_1_id}, {snapshot_2_id}); -- **DuckLake Deletions**\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n[Kontrolle: Neu hinzugefügte Einträge (Ablenkung)]\\n\")\n",
    "con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        ip, \n",
    "        url,\n",
    "        status_code \n",
    "    FROM \n",
    "        ducklake_table_insertions('{CATALOG_NAME}', 'main', '{table_name}', {snapshot_1_id}, {snapshot_2_id}); -- **DuckLake Insertions**\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"Die `deletions`-Funktion rekonstruiert exakt die von der Manipulation entfernten Beweismittel (alle Statuscodes >= 400).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Time Travel: Rekonstruktion der Top-IPs vor der Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Analyse des aktuellen (manipulierten) Zustands (Snapshot 2) ---\n",
    "top_ips_current = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        ip, \n",
    "        COUNT(*) AS cnt\n",
    "    FROM \n",
    "        {table_name}\n",
    "    GROUP BY \n",
    "        ip\n",
    "    ORDER BY \n",
    "        cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# --- 2. Analyse des historischen (Original-) Zustands (Snapshot 1) mit Time Travel ---\n",
    "top_ips_historic = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        ip, \n",
    "        COUNT(*) AS cnt\n",
    "    FROM \n",
    "        {table_name} FOR TIMESTAMP AS OF '{snapshot_1_time}' -- **DuckLake Time Travel**\n",
    "    GROUP BY \n",
    "        ip\n",
    "    ORDER BY \n",
    "        cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "### Visualisierung des Beweises\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Aktueller, manipulierter Zustand\n",
    "top_ips_current.plot.bar(x=\"ip\", y=\"cnt\", legend=False, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Top IPs (Aktueller Zustand - Snapshot 2)\")\n",
    "axes[0].set_ylabel(\"Anzahl der Anfragen\")\n",
    "axes[0].set_xlabel(\"IP-Adresse\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Historischer, rekonstruierter Zustand (Beweis)\n",
    "top_ips_historic.plot.bar(x=\"ip\", y=\"cnt\", legend=False, ax=axes[1], color='darkred')\n",
    "axes[1].set_title(\"Top IPs (Zustand vor Manipulation - Snapshot 1)\")\n",
    "axes[1].set_ylabel(\"Anzahl der Anfragen\")\n",
    "axes[1].set_xlabel(\"IP-Adresse\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vertiefende Forensische Analysen mit Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Anomalie-Erkennung: Statuscode-Verteilung im Vergleich\n",
    "Der Vergleich der Statuscodes belegt den Manipulationsversuch quantitativ: Die Fehlercodes sind im Original-Log deutlich vorhanden, im aktuellen Log aber fast vollständig verschwunden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_status_codes(query_suffix, title_suffix):\n",
    "    \"\"\"Führt die Statuscode-Analyse auf dem aktuellen oder historischen Zustand durch.\"\"\"\n",
    "    status_analysis = con.execute(f\"\"\"\n",
    "        SELECT \n",
    "            CAST(status_code AS VARCHAR) AS status_code, \n",
    "            COUNT(*) AS cnt\n",
    "        FROM \n",
    "            {table_name} {query_suffix}\n",
    "        GROUP BY \n",
    "            status_code\n",
    "        ORDER BY \n",
    "            cnt DESC\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    status_analysis.plot.bar(x=\"status_code\", y=\"cnt\", legend=False, \n",
    "                             title=f\"HTTP-Statuscode Verteilung ({title_suffix})\", \n",
    "                             figsize=(8, 4), \n",
    "                             color=['darkred' if int(s) >= 400 else 'green' for s in status_analysis['status_code']])\n",
    "    plt.ylabel(\"Anzahl der Anfragen\")\n",
    "    plt.xlabel(\"Statuscode\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "    print(f\"Statuscode-Analyse ({title_suffix}):\")\n",
    "    print(status_analysis)\n",
    "\n",
    "# 1. Analyse des manipulierten Zustands (aktuell)\n",
    "analyze_status_codes(\"\", \"Aktueller Zustand (Manipuliert)\")\n",
    "\n",
    "# 2. Analyse des Original-Zustands (Time Travel)\n",
    "analyze_status_codes(f\"FOR TIMESTAMP AS OF '{snapshot_1_time}'\", \"Historischer Zustand (Original - Beweis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Analyse der Häufig aufgerufenen URLs (Entfernte Ziele)\n",
    "Die Überprüfung der Top-URLs im historischen Log kann fehlende kritische Ziele, die vor der Manipulation aufgerufen wurden, aufdecken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_urls(query_suffix, title_suffix):\n",
    "    top_urls = con.execute(f\"\"\"\n",
    "        SELECT \n",
    "            url, \n",
    "            COUNT(*) AS cnt\n",
    "        FROM \n",
    "            {table_name} {query_suffix}\n",
    "        GROUP BY \n",
    "            url\n",
    "        ORDER BY \n",
    "            cnt DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    top_urls.plot.barh(x=\"url\", y=\"cnt\", legend=False, \n",
    "                      title=f\"Top 10 Aufgerufene URLs ({title_suffix})\", \n",
    "                      figsize=(10, 5), color='darkgreen')\n",
    "    plt.xlabel(\"Anzahl der Anfragen\")\n",
    "    plt.ylabel(\"URL\")\n",
    "    plt.gca().invert_yaxis() \n",
    "    plt.show()\n",
    "    print(f\"Top-URL-Analyse ({title_suffix}):\")\n",
    "    print(top_urls)\n",
    "\n",
    "# 1. Analyse des manipulierten Zustands (aktuell)\n",
    "analyze_top_urls(\"\", \"Aktueller Zustand\")\n",
    "\n",
    "# 2. Analyse des Original-Zustands (Time Travel)\n",
    "analyze_top_urls(f\"FOR TIMESTAMP AS OF '{snapshot_1_time}'\", \"Historischer Zustand (Original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Zeitliche Analyse des Gesamttraffics (Requests pro Tag)\n",
    "Wir nutzen Time Travel, um den ursprünglichen Traffic-Verlauf zu rekonstruieren und festzustellen, ob die Manipulation eine zeitliche Anomalie in den Gesamt-Requests verursacht hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des Traffic-Verlaufs im unmanipulierten Log (Snapshot 1)\n",
    "requests_per_day_historic = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        CAST(timestamp AS DATE) AS day, \n",
    "        COUNT(*) AS cnt\n",
    "    FROM \n",
    "        {table_name} FOR TIMESTAMP AS OF '{snapshot_1_time}' -- Time Travel\n",
    "    GROUP BY \n",
    "        day\n",
    "    ORDER BY \n",
    "        day\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Requests pro Tag (Historisch - Vor der Manipulation):\\n\")\n",
    "print(requests_per_day_historic)\n",
    "\n",
    "requests_per_day_historic.plot(x=\"day\", y=\"cnt\", kind=\"line\", title=\"Requests pro Tag (Historisch)\", figsize=(10, 5))\n",
    "plt.ylabel(\"Anzahl der Requests\")\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Abschluss und Beweissicherung\n",
    "Die DuckLake-Funktionen haben es ermöglicht, **unveränderliche Beweise** (gelöschte Fehler-Logs, Statuscode-Anomalie) aus dem historischen Log zu extrahieren. Das Archiv ist nun gesichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schließe die DuckDB-Verbindung und speichere alle Metadaten final ab\n",
    "con.close()\n",
    "print(\"✅ Forensische Analyse abgeschlossen. Das Archiv ist gesichert.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}