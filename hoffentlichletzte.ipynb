{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjvEcpeWJ6Du"
   },
   "source": [
    "# üïµÔ∏è Forensische Datenanalyse von Weblogs mit DuckLake (DuckDB)",
    "",
    "Dieses Notebook dokumentiert die forensische Untersuchung von Weblog-Daten zur Identifizierung und zum Beweis einer simulierten **Datenmanipulation**.",
    "",
    "Wir nutzen **DuckLake** (eine Erweiterung f√ºr DuckDB), um die Unver√§nderlichkeit der Beweiskette (**Chain of Custody**) zu gew√§hrleisten. DuckLake erstellt f√ºr jede Schreiboperation einen unver√§nderlichen **Snapshot** (Version), was das **Time Travel** zur Rekonstruktion des Originalzustands erm√∂glicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtZMQQyaJ6Dw"
   },
   "source": [
    "---",
    "",
    "## 1. Zweck und Ziele der Untersuchung",
    "",
    "### 1.1 Untersuchungsziele",
    "",
    "Ziel der Untersuchung ist es, folgende Fragen anhand der Weblog-Daten zu beantworten:",
    "",
    "1.  **Beweissicherung:** Erfolgreiche Erfassung des digitalen Beweisst√ºcks (Logdatei) als unver√§nderlichen **Snapshot 1 (Original)**.",
    "2.  **Hypothese der Manipulation:** Simulation und Nachweis der L√∂schung von Log-Eintr√§gen (Statuscodes >= 400).",
    "3.  **Beweisf√ºhrung:** Wiederherstellung des Originalzustands (Time Travel) und Isolierung der gel√∂schten Datens√§tze mittels der √Ñnderungsverfolgung von DuckLake (`ducklake_table_deletions`).",
    "",
    "### 1.2 Datenakquisition und Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g9tE_q_J6Dx"
   },
   "outputs": [],
   "source": [
    "# Installation der notwendigen Bibliotheken (Kaggle f√ºr Daten-Download/Annahme)",
    "! pip install -q duckdb pandas matplotlib",
    "! pip install -q kaggle # Nur n√∂tig, wenn Daten via Kaggle API geladen werden",
    "",
    "# Erstellung eines Datenverzeichnisses und Entpacken der Log-Daten (Annahme: web-log-dataset.zip ist vorhanden)",
    "# **WICHTIG:** Stellen Sie sicher, dass die Datei `weblog.csv` im Verzeichnis `./data/` existiert.",
    "! mkdir -p ./data",
    "# ! unzip web-log-dataset.zip -d ./data # F√ºhren Sie dies aus, wenn Sie die ZIP-Datei haben",
    "",
    "# Laden der Bibliotheken und Definition der Pfade",
    "import os",
    "import time",
    "import pandas as pd",
    "import duckdb",
    "import shutil",
    "",
    "# Pfad zur Weblog-Datei (Annahme: weblog.csv ist der Datensatz)",
    "LOG_PATH = './data/weblog.csv'",
    "",
    "# --- Konfiguration der forensischen Datenbankpfade (DuckLake) ---",
    "# DuckLake speichert Metadaten und Snapshots in diesen Pfaden.",
    "DUCKLAKE_METADATA_PATH = 'forensic_evidence.ducklake'",
    "DUCKLAKE_DATA_PATH = 'forensic_data_files'",
    "CATALOG_NAME = 'forensic_log_archive'",
    "table_name = 'access_logs'",
    "",
    "# Erstellung der Verzeichnisse",
    "os.makedirs(DUCKLAKE_DATA_PATH, exist_ok=True)",
    "print(f\"‚úÖ Lokale Verzeichnisse f√ºr forensische Beweise erstellt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVfVdJmEJ6Dy"
   },
   "source": [
    "### 1.3 Datenbereinigung und Ingestion (Beweissicherung: Snapshot 1)",
    "",
    "Die Rohdaten werden in Pandas geladen, bereinigt und anschlie√üend in den DuckLake-Katalog als erster unver√§nderlicher Zustand (*Snapshot 1*) eingef√ºgt. Dies sichert den **Original-Beweis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o2Jm7k2J6Dz"
   },
   "outputs": [],
   "source": [
    "# --- Datenbereinigung und Transformation ---",
    "try:",
    "    # 1. Daten einlesen",
    "    df = pd.read_csv(LOG_PATH)",
    "    ",
    "    # 2. Spaltennamen standardisieren",
    "    df.columns = df.columns.str.strip().str.lower()",
    "    ",
    "    # 3. Zeitstempel konvertieren (entfernt '[', nutzt Apache Log Format)",
    "    df['time'] = pd.to_datetime(",
    "        df['time'].astype(str).str.replace(\"[\", \"\", regex=False),",
    "        format=\"%d/%b/%Y:%H:%M:%S\",",
    "        errors=\"coerce\" ",
    "    )",
    "    ",
    "    # 4. Spaltenumbenennung ('staus' -> 'status_code' und 'time' -> 'timestamp')",
    "    df.rename(columns={'time': 'timestamp', 'staus': 'status_code'}, inplace=True)",
    "    ",
    "    # 5. Statuscodes in robusten Integer-Typ konvertieren",
    "    df['status_code'] = pd.to_numeric(",
    "        df['status_code'],",
    "        errors='coerce' ",
    "    ).astype(pd.Int64Dtype())",
    "    ",
    "    # 6. Finale Spaltenauswahl f√ºr die Archivierung",
    "    df = df[['timestamp', 'ip', 'url', 'status_code']]",
    "    print(f\"‚úÖ Rohdaten erfolgreich bereinigt. {len(df)} Eintr√§ge bereit zur Archivierung.\")",
    "",
    "except FileNotFoundError:",
    "    print(f\"FEHLER: Datei nicht gefunden unter {LOG_PATH}. Bitte stellen Sie sicher, dass `weblog.csv` im Verzeichnis `./data/` liegt.\")",
    "    raise",
    "",
    "# --- DuckLake Initialisierung und Snapshot 1 (Beweissicherung) ---",
    "con = duckdb.connect(database='forensic_duckdb.db') # Lokale DuckDB-Datenbank",
    "con.sql(\"INSTALL ducklake;\")",
    "con.sql(\"LOAD ducklake;\")",
    "",
    "# Katalog anh√§ngen und verwenden (definiert den Speicherort der Snapshots)",
    "attach_query = f\"ATTACH 'ducklake:{DUCKLAKE_METADATA_PATH}' AS {CATALOG_NAME} (DATA_PATH '{DUCKLAKE_DATA_PATH}');\"",
    "con.sql(attach_query)",
    "con.sql(f\"USE {CATALOG_NAME};\")",
    "",
    "# 1. Leere Zieltabelle im DuckLake erstellen",
    "con.sql(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df LIMIT 0;\")",
    "",
    "# 2. Daten einf√ºgen (erzeugt Snapshot 1: Original-Beweis)",
    "con.sql(f\"INSERT INTO {table_name} SELECT * FROM df;\")",
    "",
    "# 3. Snapshot ID 1 zur sp√§teren Time Travel Verwendung speichern",
    "snapshot_1_id = con.sql(f\"SELECT max(snapshot_id) FROM ducklake_snapshots('{CATALOG_NAME}')\").fetchone()[0]",
    "",
    "print(f\"\\n‚úÖ Snapshot 1 (Original-Log) erfasst. ID: {snapshot_1_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ2eN-NlKk4S"
   },
   "source": [
    "---",
    "",
    "## 2. Analyse auf Anomalien, au√üergew√∂hnliche Aktivit√§ten und Beweise",
    "",
    "### 2.1 Simulation der Manipulation (Erzeugt Snapshot 2)",
    "",
    "Wir simulieren, wie ein Angreifer kritische Log-Eintr√§ge entfernt und einen Ablenkungseintrag hinzuf√ºgt. Diese Aktionen f√ºhren zum **Snapshot 2**, dem manipulierten Zustand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6X9wVlqKk4U"
   },
   "outputs": [],
   "source": [
    "# Kurze Pause, um einen eindeutigen Zeitstempel f√ºr den Snapshot 2 zu gew√§hrleisten",
    "time.sleep(1) ",
    "",
    "# --- KRITISCHE AKTION 1: L√∂schen von Fehlereintr√§gen (Simulierte Manipulation) ---",
    "# Entfernung aller kritischen HTTP-Fehler (4xx und 5xx)",
    "deleted_count = con.execute(f\"DELETE FROM {table_name} WHERE status_code >= 400;\").fetchone()[0]",
    "print(f\" ‚ùå {deleted_count} Fehler-Eintr√§ge (>= 400) wurden gel√∂scht (Simulierte Manipulation).\")",
    "",
    "# --- KRITISCHE AKTION 2: Hinzuf√ºgen eines Ablenkungseintrags ---",
    "new_log_df = pd.DataFrame({",
    " 'timestamp': [pd.to_datetime(time.time(), unit='s')], ",
    " 'ip': ['203.0.113.5'],",
    " 'url': ['/index.html'],",
    " 'status_code': [200] ",
    "})",
    "con.sql(f\"INSERT INTO {table_name} SELECT * FROM new_log_df;\")",
    "print(f\" ‚úÖ Ein neuer 200-Eintrag wurde hinzugef√ºgt (Ablenkung).\")",
    "",
    "# Snapshot 2 ID speichern",
    "snapshot_2_id = con.sql(f\"SELECT max(snapshot_id) FROM ducklake_snapshots('{CATALOG_NAME}')\").fetchone()[0]",
    "print(f\"\\n‚úÖ Snapshot 2 (Manipuliertes Log) erfasst. ID: {snapshot_2_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1k_T-22Kk4W"
   },
   "source": [
    "### 2.2 Nachweis der Manipulation: Vergleichsanalyse",
    "",
    "Wir nutzen **Time Travel**, um die Statuscodes im **Original-Log (Snapshot 1)** mit dem **Manipulierten Log (Snapshot 2)** zu vergleichen und die Diskrepanz als Anomalie zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-dO0j-oKk4X"
   },
   "outputs": [],
   "source": [
    "# --- Analyse 1: Statuscodes im ORIGINAL-LOG (Snapshot 1) ---",
    "print(\"\\n--- Z√§hlungen im ORIGINAL-LOG (Snapshot 1) ---\")",
    "con.sql(f\"\"",
    "    SELECT ",
    "        status_code, ",
    "        count(*) AS anzahl,",
    "        round(count(*) * 100.0 / (SELECT count(*) FROM {CATALOG_NAME}.ducklake_table_at_snapshot('{table_name}', {snapshot_1_id})), 2) AS anteil_prozent",
    "    FROM ",
    "        {CATALOG_NAME}.ducklake_table_at_snapshot('{table_name}', {snapshot_1_id}) -- Wichtige Time Travel Funktion!",
    "    GROUP BY ",
    "        status_code",
    "    ORDER BY ",
    "        anzahl DESC;",
    "\"\").show()",
    "",
    "# --- Analyse 2: Statuscodes im MANIPULIERTEN LOG (Snapshot 2) ---",
    "print(\"\\n--- Z√§hlungen im MANIPULIERTEN LOG (Snapshot 2 - Aktueller Stand) ---\")",
    "con.sql(f\"\"",
    "    SELECT ",
    "        status_code, ",
    "        count(*) AS anzahl",
    "    FROM ",
    "        {table_name} -- Hier wird der aktuelle (manipulierte) Zustand abgefragt",
    "    GROUP BY ",
    "        status_code",
    "    ORDER BY ",
    "        anzahl DESC;",
    "\"\").show()",
    "",
    "# **Interpretation:** Im manipulierten Log (Snapshot 2) fehlen alle Fehler (z.B. 404/400/403), die im Original-Log (Snapshot 1) vorhanden waren. Dies ist der erste Beweis der Anomalie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP5tU3dDKk4Y"
   },
   "source": [
    "### 2.3 Ermittlung der gel√∂schten Beweise (Kritische Beweisf√ºhrung)",
    "",
    "Die Funktion **`ducklake_table_deletions`** ist das Herzst√ºck der forensischen Analyse. Sie erlaubt die Rekonstruktion jeder gel√∂schten Zeile im √úbergang zu einem bestimmten Snapshot. Dies liefert den **unwiderlegbaren Beweis** der Manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9p0e9t0Kk4Z"
   },
   "outputs": [],
   "source": [
    "# --- Forensischer Beweis: Wiederherstellung der GEL√ñSCHTEN DATENS√ÑTZE ---",
    "print(\"\\n--- FORENSISCHER BEWEIS: Die gel√∂schten Log-Eintr√§ge ---\")",
    "con.sql(f\"\"",
    "    SELECT ",
    "        deleted_timestamp AS Zeitstempel_der_Loeschung, ",
    "        status_code, ",
    "        ip, ",
    "        url",
    "    FROM ",
    "        {CATALOG_NAME}.ducklake_table_deletions('{table_name}')",
    "    WHERE ",
    "        snapshot_id = {snapshot_2_id} -- Zeigt alle L√∂schungen, die den √úbergang zu Snapshot 2 verursacht haben",
    "    LIMIT 10; -- Zeige die ersten 10 gel√∂schten Eintr√§ge",
    "\"\").show()",
    "",
    "# --- Statistische Analyse der gel√∂schten Eintr√§ge (Beweis der Absicht) ---",
    "print(\"\\n--- Z√§hlung der gel√∂schten Statuscodes (Beweis der Absicht) ---\")",
    "con.sql(f\"\"",
    "    SELECT ",
    "        status_code, ",
    "        count(*) AS anzahl_geloescht",
    "    FROM ",
    "        {CATALOG_NAME}.ducklake_table_deletions('{table_name}')",
    "    WHERE ",
    "        snapshot_id = {snapshot_2_id}",
    "    GROUP BY ",
    "        status_code",
    "    ORDER BY ",
    "        anzahl_geloescht DESC;",
    "\"\").show()",
    "",
    "# **Interpretation:** Die `ducklake_table_deletions` Tabelle liefert die **vollst√§ndigen Daten** der gel√∂schten Eintr√§ge, was die urspr√ºngliche `DELETE`-Aktion und die Manipulation zweifelsfrei beweist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9X8O4t_Kk4a"
   },
   "source": [
    "### 2.4 Baseline-Analyse: Top IPs und URLs (Snapshot 1)",
    "",
    "Zus√§tzlich zur Beweisf√ºhrung identifizieren wir die Top-Akteure und -Ressourcen im **unver√§nderten Original-Log (Snapshot 1)**, um eine Baseline f√ºr weitere Untersuchungen zu schaffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6S9iNlJKk4b"
   },
   "outputs": [],
   "source": [
    "# --- Analyse 3: Top 10 IP-Adressen (Original-Log: Snapshot 1) ---",
    "print(\"\\n--- Top 10 IP-Adressen (Original-Log) ---\")",
    "con.sql(f\"\"",
    "    SELECT ",
    "        ip, ",
    "        count(*) AS anzahl",
    "    FROM ",
    "        {CATALOG_NAME}.ducklake_table_at_snapshot('{table_name}', {snapshot_1_id}) ",
    "    GROUP BY ",
    "        ip",
    "    ORDER BY ",
    "        anzahl DESC",
    "    LIMIT 10;",
    "\"\").show()",
    "",
    "",
    "# --- Analyse 4: Top 10 aufgerufene URLs (Original-Log: Snapshot 1) ---",
    "print(\"\\n--- Top 10 aufgerufene URLs (Original-Log) ---\")",
    "con.sql(f\"\"",
    "    SELECT ",
    "        url, ",
    "        count(*) AS anzahl",
    "    FROM ",
    "        {CATALOG_NAME}.ducklake_table_at_snapshot('{table_name}', {snapshot_1_id}) ",
    "    GROUP BY ",
    "        url",
    "    ORDER BY ",
    "        anzahl DESC",
    "    LIMIT 10;",
    "\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T4H_43aKk4c"
   },
   "source": [
    "---",
    "",
    "## 3. Dokumentation und Abschluss",
    "",
    "### 3.1 Unver√§nderliche Historie (Audit-Log)",
    "",
    "Die Tabelle `ducklake_snapshots` dient als manipulationssicheres Audit-Log und dokumentiert jede √Ñnderung an den Log-Daten (einschlie√ülich der Ingestion und der Manipulation). Sie bildet die **Chain of Custody** ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_zYt4FKKk4d"
   },
   "outputs": [],
   "source": [
    "# Abfrage der gesamten Snapshot-Historie als Audit-Log",
    "con.sql(f\"\"",
    "    SELECT",
    "        snapshot_id,",
    "        strftime(snapshot_time, '%Y-%m-%d %H:%M:%S') AS snapshot_time,",
    "        changes, -- Zeigt die Netto-√Ñnderung in Zeilen (+/-)",
    "        total_rows",
    "    FROM",
    "        ducklake_snapshots('{CATALOG_NAME}')",
    "    ORDER BY",
    "        snapshot_id;",
    "\"\").show()",
    "",
    "# **Erkl√§rung:** Die Historie zeigt:",
    "# - **Snapshot 1:** Die urspr√ºngliche Ingestion (z.B. +16007 Zeilen).",
    "# - **Snapshot 2:** Die simulierte Manipulation (zeigt die Netto√§nderung aus L√∂schung und Einf√ºgung, z.B. -251 Zeilen (gel√∂scht) + 1 Zeile (neu))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD-R2398Kk4e"
   },
   "source": [
    "### 3.2 Bereinigung der Umgebung",
    "",
    "Nach Abschluss der forensischen Analyse werden die tempor√§ren Datenbank- und Dateisystemobjekte entfernt, um eine saubere Arbeitsumgebung zu gew√§hrleisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lG1oV9vKk4f"
   },
   "outputs": [],
   "source": [
    "# Schlie√üe die DuckDB-Verbindung",
    "con.close()",
    "",
    "# Entferne die tempor√§r erstellten forensischen Artefakte und Daten (DuckDB- und DuckLake-Dateien)",
    "if os.path.exists('forensic_duckdb.db'):",
    "    os.remove('forensic_duckdb.db')",
    "",
    "if os.path.exists(DUCKLAKE_METADATA_PATH):",
    "    os.remove(DUCKLAKE_METADATA_PATH)",
    "",
    "if os.path.exists(DUCKLAKE_DATA_PATH):",
    "    shutil.rmtree(DUCKLAKE_DATA_PATH)",
    "",
    "print(\"‚úÖ Forensische Umgebung (Datenbank-Dateien und DuckLake-Katalog) erfolgreich bereinigt.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}